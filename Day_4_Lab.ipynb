{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prekshya-dawadi/GDLLabs/blob/main/Day_4_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self supervised Learning Lab\n",
        "\n",
        "This lab focuses on self-supervised contrastive learning, a method for extracting meaningful features from unlabeled data. Self-supervised learning avoids the need for manual labeling by leveraging patterns and differences within the data itself, enabling models to adapt quickly to downstream tasks. For instance, in autonomous driving, large datasets can be collected by mounting a camera in a car, saving significant time and cost compared to manual annotation.\n",
        "\n",
        "Recent advancements in self-supervised learning, particularly in contrastive learning, have demonstrated impressive performance with minimal labeled data. Contrastive learning trains models to cluster augmented versions of an image while maximizing their separation from other images. A notable method, SimCLR, uses augmentations like cropping, noise, and blurring, combined with a CNN (e.g., ResNet), to produce robust feature representations. (figure credit - [Ting Chen et al.](https://simclr.github.io/)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/simclr_contrastive_learning.png?raw=1\" width=\"500px\"></center>\n",
        "\n",
        "The framework involves creating augmented versions of unlabeled images, processing them through a network, and training the outputs to align closely for augmented pairs while diverging for unrelated images. This helps the model focus on invariant content, such as objects, that are essential for downstream tasks.\n",
        "\n",
        "The lab will compare downstream task performance between:\n",
        "\n",
        "- A model trained using feature extractor initialized with SimCLR-pretrained feature extractors (trained in a self-supervised manner).\n",
        "- A model trained using feature extractor initialized with fully supervised training (e.g., on a labeled dataset like ImageNet).\n",
        "\n",
        "This comparison evaluates the effectiveness of SimCLR-pretrained features and their ability to serve as a foundation for task-specific fine-tuning, offering an efficient alternative to fully supervised training.\n",
        "\n",
        "This lab is heavily inspired by and adapted from the labs provided in the \"Lecture Series on Deep Learning\" at the University of Amsterdam."
      ],
      "metadata": {
        "id": "QxtQsG7jFjTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MeJ18faiwasB",
        "outputId": "2eb1b2e8-1ce7-46d4-8f56-f4574d2cf13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Device: cuda:0\n",
            "Number of workers: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e57d2610f6ae>:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#numpy\n",
        "import numpy as np\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision import transforms\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial17\"\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Setting the seed\n",
        "\n",
        "fix_seed(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hD9GAwsyFgvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"SimCLR.ckpt\", \"ResNet.ckpt\",\n",
        "                    \"tensorboards/SimCLR/events.out.tfevents.SimCLR\",\n",
        "                    \"tensorboards/classification/ResNet/events.out.tfevents.ResNet\"]\n",
        "pretrained_files += [f\"LogisticRegression_{size}.ckpt\" for size in [10, 20, 50, 100, 200, 500]]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ],
      "metadata": {
        "id": "t-BRadBkw9-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```ContrastiveTransformations``` class generates multiple augmented versions of an image for SimCLR. In SimCLR, two augmented views of the same image are created to form positive pairs for training.\n",
        "\n",
        "- Base Transforms: A set of data augmentations (e.g., cropping, color distortion) applied to the image.\n",
        "- Number of Views (n_views): Specifies how many augmented versions to generate (typically 2 for SimCLR).\n",
        "- Functionality: When called, the class applies the augmentations multiple times to create a list of augmented views.\n",
        "\n",
        "This is used in SimCLR to train the model to recognize similarities between augmented views of the same image while distinguishing them from other images."
      ],
      "metadata": {
        "id": "3fNWfzYuJW3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveTransformations(object):\n",
        "    def __init__(self, base_transforms, n_views=2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x) for i in range(self.n_views)]"
      ],
      "metadata": {
        "id": "-w7RATUGxT5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrast_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.RandomResizedCrop(size=96),\n",
        "                                          transforms.RandomApply([\n",
        "                                              transforms.ColorJitter(brightness=0.5,\n",
        "                                                                     contrast=0.5,\n",
        "                                                                     saturation=0.5,\n",
        "                                                                     hue=0.1)\n",
        "                                          ], p=0.8),\n",
        "                                          transforms.RandomGrayscale(p=0.2),\n",
        "                                          transforms.GaussianBlur(kernel_size=9),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((0.5,), (0.5,))\n",
        "                                         ])"
      ],
      "metadata": {
        "id": "sc_HmHbxxcqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block loads the **STL-10 dataset** and applies contrastive transformations to prepare it for SimCLR training. The unlabeled split is used for self-supervised pretraining, and the labeled training split is prepared similarly for later evaluation. Each image is augmented twice to generate positive pairs for contrastive learning.[link text](https://)"
      ],
      "metadata": {
        "id": "qi009G02JvAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data = STL10(root=DATASET_PATH, split='unlabeled', download=True,\n",
        "                       transform=ContrastiveTransformations(contrast_transforms, n_views=2))\n",
        "train_data_contrast = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                            transform=ContrastiveTransformations(contrast_transforms, n_views=2))"
      ],
      "metadata": {
        "id": "ouBstXg4xeyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we visualize examples of augmented images from the STL-10 dataset. We generate two augmented versions for each image using the contrastive transformations and display them in a grid. This allows us to see the variety and diversity introduced by the augmentation process, which is crucial for contrastive learning."
      ],
      "metadata": {
        "id": "56JqHOnAJ6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some examples\n",
        "fix_seed(42)\n",
        "NUM_IMAGES = 6\n",
        "imgs = torch.stack([img for idx in range(NUM_IMAGES) for img in unlabeled_data[idx][0]], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Augmented image examples of the STL10 dataset')\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "RBPclHCgxgk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBp_XfLkKPTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a CNN model, Resnet18SimCLR, based on ResNet-18 to use for SimCLR. The model includes:\n",
        "\n",
        "Backbone CNN: ResNet-18 is used as the base feature extractor, with its fully connected layer (fc) replaced.\n",
        "\n",
        "Projection Head: The modified fc layer consists of two linear layers with a ReLU activation in between. It maps the ResNet features into a lower-dimensional space, as required for contrastive learning.\n",
        "This architecture is designed to extract features from input images and project them into a representation space where SimCLR can effectively learn similarities and differences between augmented image pairs."
      ],
      "metadata": {
        "id": "gnmjKmlZLG2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Resnet18SimCLR(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Resnet18SimCLR, self).__init__()\n",
        "\n",
        "        # Backbone CNN\n",
        "        self.convnet = torchvision.models.resnet18(num_classes=4 * hidden_dim)\n",
        "        # cnn_dim = self.cnn_dim = self.cnn.fc.in_features\n",
        "\n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        out = self.conv_net(image)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "oza1WGTfxjiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_resnet = Resnet18SimCLR(128)"
      ],
      "metadata": {
        "id": "ILr0pWSW1zDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we donâ€™t have sufficient computational resources to train the model from scratch using self-supervised learning, we will load a pre-trained checkpoint. This checkpoint contains a model that has already been trained on the STL-10 unlabeled dataset using SimCLR. This allows us to skip the training process and directly use the pre-trained model for downstream tasks or further evaluation"
      ],
      "metadata": {
        "id": "APXBzU-lNDWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
        "simclr_ckpt = torch.load(pretrained_filename)"
      ],
      "metadata": {
        "id": "IXRFvh9C1_Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_resnet.load_state_dict(simclr_ckpt['state_dict'])"
      ],
      "metadata": {
        "id": "Tvdh3WwCU58C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Class Description\n",
        "\n",
        "This code defines a class `CNN`, which is a neural network based on ResNet-18, designed for classification tasks. The architecture includes the following components:\n",
        "\n",
        "- **Backbone CNN**:\n",
        "  - A ResNet-18 model is used to extract features from the input image.\n",
        "  - The fully connected (`fc`) layer of ResNet is replaced with an identity layer, meaning it outputs the raw feature embeddings instead of predictions.\n",
        "\n",
        "- **Custom Classification Layer**:\n",
        "  - A new fully connected layer (`self.fc`) is added to map the extracted features to the desired number of classes (`num_classes`).\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - The image is passed through the ResNet backbone to extract feature embeddings.\n",
        "  - The extracted features are then passed through the custom classification layer to produce class predictions.\n",
        "  - Both the feature embeddings (`feat`) and the final class outputs (`out`) are returned.\n",
        "\n",
        "This setup allows for flexible use of the feature embeddings, which can be utilized for additional tasks or analysis alongside the classification output.\n"
      ],
      "metadata": {
        "id": "V-jJOIjuOCsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Backbone CNN\n",
        "        self.convnet = torchvision.models.resnet18(num_classes=num_classes, pretrained=False)\n",
        "        # cnn_dim = self.cnn_dim = self.cnn.fc.in_features\n",
        "        self.convnet.fc = nn.Identity()\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        feat = self.convnet(image)\n",
        "        out = self.fc(feat)\n",
        "        return feat, out\n"
      ],
      "metadata": {
        "id": "cacg5ePf2o5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_network = CNN(hidden_dim=512, num_classes=10)"
      ],
      "metadata": {
        "id": "jWnMUbDN3A-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_simclr_weights(cnn, simclr_cnn):\n",
        "  cnn_state_dict = cnn.state_dict()\n",
        "  simclr_state_dict = simclr_cnn.state_dict()\n",
        "\n",
        "  # Transfer only shared layer weights (ignoring fc layers)\n",
        "  for name, param in simclr_state_dict.items():\n",
        "      if name in cnn_state_dict and 'fc' not in name:  # Exclude fc layers\n",
        "          # print(\"name\")\n",
        "          cnn_state_dict[name] = param\n",
        "  cnn.load_state_dict(cnn_state_dict)\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "DfI9uSgA5i_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The snippet initializes two models for further use. First, it transfers the pre-trained SimCLR weights from the simclr_resnet model to cnn_network using the load_simclr_weights function, adapting the SimCLR backbone for feature extraction. Then, it creates an instance of the CNN class with a hidden dimension of 512 and 10 output classes, setting up a model for supervised classification tasks."
      ],
      "metadata": {
        "id": "JfNMGXlsO_Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simclr_resnet.load_state_dict()\n",
        "cnn_network = load_simclr_weights(cnn_network, simclr_resnet)"
      ],
      "metadata": {
        "id": "FZe6x76w6Wk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_supervised = CNN(hidden_dim=512, num_classes=10)"
      ],
      "metadata": {
        "id": "st_0XOV5A2b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_img_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                       transform=img_transforms)\n",
        "test_img_data = STL10(root=DATASET_PATH, split='test', download=True,\n",
        "                      transform=img_transforms)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_img_data))\n",
        "print(\"Number of test examples:\", len(test_img_data))"
      ],
      "metadata": {
        "id": "7GOwZtm76f-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = data.DataLoader(train_img_data, batch_size=64, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
        "test_data_loader = data.DataLoader(test_img_data, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "Go6J-6Xu9Qpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Trainer Description\n",
        "\n",
        "The `Classification_trainer` class is designed to train, test, and manage a classification model. It streamlines the supervised learning process, including data preparation, training, evaluation, and checkpoint management. Below is an overview of its components:\n",
        "\n",
        "#### Key Components\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "   - Sets up training parameters like `max_epochs` and model information.\n",
        "   - Prepares data loaders and directories for saving model checkpoints.\n",
        "\n",
        "2. **Data Setup (`setup_dataloader`)**:\n",
        "   - Loads the **STL-10 dataset** for both training and testing.\n",
        "   - Applies transformations such as normalization and tensor conversion.\n",
        "   - Creates data loaders for efficient batch processing.\n",
        "\n",
        "3. **Training Setup (`setup_training`)**:\n",
        "   - Configures:\n",
        "     - Optimizer: `AdamW` with learning rate and weight decay.\n",
        "     - Loss function: Cross-entropy for classification.\n",
        "     - Learning rate scheduler to adjust the learning rate at specific milestones.\n",
        "\n",
        "4. **Training (`train_one_epoch`)**:\n",
        "   - Trains the model for one epoch by:\n",
        "     - Iterating through the training data.\n",
        "     - Calculating losses and performing backpropagation.\n",
        "     - Updating model weights.\n",
        "\n",
        "5. **Testing (`test`)**:\n",
        "   - Evaluates the model on the test dataset.\n",
        "   - Computes accuracy by comparing predictions with ground-truth labels.\n",
        "   - Tracks and returns the test accuracy for analysis.\n",
        "\n",
        "6. **Training Loop (`run`)**:\n",
        "   - Manages the full training process, including:\n",
        "     - Training and testing over all epochs.\n",
        "     - Tracking and saving the best-performing model based on accuracy.\n",
        "     - Adjusting the learning rate as per the scheduler.\n",
        "\n",
        "7. **Model Saving and Loading**:\n",
        "   - `save_best()`: Saves the model's weights and the epoch when it achieved the best performance.\n",
        "   - `load_best()`: Loads the best-performing model for final evaluation or further use.\n",
        "\n",
        "8. **Accuracy Visualization (`plot_accs`)**:\n",
        "   - Plots the model's accuracy over epochs to provide insights into its performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Task: Complete the Training Loop, Setup, and DataLoader\n",
        "\n",
        "In this task, you will complete the missing parts of the training process. Specifically, you need to:\n",
        "\n",
        "1. **Complete the Training Loop (`train_one_epoch`)**:\n",
        "2. **Complete the Training Setup (`setup_training`)**:\n",
        "\n",
        "\n",
        "Use the following guidelines to complete the implementation:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Training Loop (`train_one_epoch`)\n",
        "The loop should:\n",
        "   - Set the model to training mode (`model.train()`).\n",
        "   - Iterate through the training DataLoader.\n",
        "   - Perform the following for each batch:\n",
        "     - Load the images and labels to the GPU.\n",
        "     - Perform a forward pass to obtain predictions.\n",
        "     - Compute the loss using the loss function.\n",
        "     - Backpropagate the gradients using `loss.backward()`.\n",
        "     - Update model weights with the optimizer.\n",
        "   - Optionally, display the loss during training for progress tracking.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Training Setup (`setup_training`)\n",
        "Define:\n",
        "   - **Optimizer**: Use `torch.optim.AdamW` or any other optimizer suitable for your task.\n",
        "   - **Loss Function**: Use `torch.nn.CrossEntropyLoss` for multi-class classification.\n",
        "---\n"
      ],
      "metadata": {
        "id": "eG3YLzOOPOIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Classification_trainer:\n",
        "  def __init__(self, hparams, model, name):\n",
        "    self.hparams = hparams\n",
        "    self.model = model\n",
        "    self.best_model = None\n",
        "    self.name = name\n",
        "    self.cur_epoch = 0\n",
        "    self.max_epoch = hparams['max_epochs']\n",
        "\n",
        "    self.setup_dataloader()\n",
        "    self.setup_training()\n",
        "    self.test_accs = []\n",
        "    self.save_dir = os.path.join(\"saved_models\", name)\n",
        "\n",
        "    if not os.path.exists(self.save_dir):\n",
        "      os.makedirs(self.save_dir)\n",
        "\n",
        "\n",
        "  def setup_dataloader(self):\n",
        "    img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_img_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                       transform=img_transforms)\n",
        "    test_img_data = STL10(root=DATASET_PATH, split='test', download=True,\n",
        "                      transform=img_transforms)\n",
        "    self.train_loader = data.DataLoader(train_img_data, batch_size=64, num_workers=NUM_WORKERS, shuffle=True, drop_last=False)\n",
        "    self.test_loader = test_data_loader = data.DataLoader(test_img_data, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "\n",
        "  def setup_training(self):\n",
        "    \"\"\"\n",
        "      Hyper-parameters are defined in dict as\n",
        "      hparams = {\n",
        "            \"lr\":1e-3,\n",
        "            \"weight_decay\":2e-4,\n",
        "            \"max_epochs\": 10\n",
        "            }\n",
        "    \"\"\"\n",
        "\n",
        "    self.optimizer = None ## Complete this Part\n",
        "    self.criterion = None ## Complete this part\n",
        "    self.lr_scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                      milestones=[int(self.hparams['max_epochs']*0.7),\n",
        "                                                                  int(self.hparams['max_epochs']*0.9)],\n",
        "                                                      gamma=0.1)\n",
        "  def train_one_epoch(self):\n",
        "    self.model.train()\n",
        "    # print(f\"Training Epoch: {self.cur_epoch}/{self.max_epochs}\")\n",
        "    with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n",
        "      for images, labels in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {self.cur_epoch}/{self.max_epoch}\")\n",
        "          #### START ######\n",
        "          ### WRITE YOUR CODE HERE ###\n",
        "          #### END #####\n",
        "        tepoch.set_postfix(Loss=losses.item())\n",
        "\n",
        "  def test(self):\n",
        "    self.model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with tqdm(self.test_loader, unit=\"batch\") as tepoch:\n",
        "      for images, targets in tepoch:\n",
        "        tepoch.set_description(f\"Testing {self.cur_epoch}/{self.max_epoch}\")\n",
        "        images = images.cuda()\n",
        "        targets = targets.cuda()\n",
        "        feats, outs = self.model(images)\n",
        "        probabilities = torch.softmax(outs,dim=1)\n",
        "        _, predicted = torch.max(probabilities,1)\n",
        "        total+=targets.size(0)\n",
        "        correct += (predicted==targets).sum().item()\n",
        "\n",
        "    cur_acc = correct/total\n",
        "    print(f\"Test accuracy is {cur_acc}\")\n",
        "    return cur_acc\n",
        "\n",
        "  def run(self):\n",
        "    self.model.cuda()\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    print(\"Training\")\n",
        "    for i in range(self.max_epoch):\n",
        "      self.train_one_epoch()\n",
        "      self.lr_scheduler.step()\n",
        "      cur_acc = self.test()\n",
        "      self.test_accs.append(cur_acc)\n",
        "\n",
        "      self.cur_epoch += 1\n",
        "\n",
        "      if cur_acc > best_acc:\n",
        "        best_acc = cur_acc\n",
        "        best_epoch = i\n",
        "        self.save_best()\n",
        "\n",
        "    self.load_best()\n",
        "    print(\"Testing best model\")\n",
        "\n",
        "    best_acc = self.test()\n",
        "    print(f\"Best Accuracy is: {best_acc} at Epoch: {best_epoch}\" )\n",
        "\n",
        "\n",
        "  def load_best(self):\n",
        "    ckpt_path = os.path.join(self.save_dir, f\"model_best.pth\")\n",
        "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    self.model.load_state_dict(checkpoint[\"net\"])\n",
        "    print(f\"Best Model is at comms : {checkpoint['epoch']}\")\n",
        "\n",
        "  def save_best(self):\n",
        "    ckpt_path = os.path.join(self.save_dir, f\"model_best.pth\")\n",
        "    torch.save({\"net\":self.model.state_dict(), \"epoch\":self.cur_epoch}, ckpt_path)\n",
        "\n",
        "  def plot_accs(self):\n",
        "    epochs = list(range(1, len(self.test_accs) + 1))\n",
        "\n",
        "    # Plot accuracy over epochs\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, self.test_accs, marker='o', linestyle='-', label='Accuracy')\n",
        "    plt.title('Accuracy over Epochs', fontsize=16)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.grid(True)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ### Training Hyper Parameter\n",
        "# LEARNING_RATE = 1e-3\n",
        "# WEIGHT_DECAY = 2e-4\n",
        "# MAX_EPOCH = 100\n",
        "# optimizer = optim.AdamW(self.parameters(),lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "### Supervised Training:\n"
      ],
      "metadata": {
        "id": "i_DuLnCn9b4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {\n",
        "    \"lr\":1e-3,\n",
        "    \"weight_decay\":2e-4,\n",
        "    \"max_epochs\": 10\n",
        "}"
      ],
      "metadata": {
        "id": "WZFRYcki9muw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "supervised_trainer = Classification_trainer(hparams, copy.deepcopy(cnn_supervised), \"Supervised\")"
      ],
      "metadata": {
        "id": "0YMjCTXI9pgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_trainer.run()"
      ],
      "metadata": {
        "id": "Xkj70Fow-FVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_trainer.plot_accs()"
      ],
      "metadata": {
        "id": "TF7qZeVFkwMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for params in cnn_network.convnet.parameters():\n",
        "#   params.requires_grad = True"
      ],
      "metadata": {
        "id": "aW_JD5Wd-aRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_supervised_trainer = Classification_trainer(hparams, copy.deepcopy(cnn_network), \"SimCLR\")"
      ],
      "metadata": {
        "id": "2hlFgaRHQH8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_supervised_trainer.run()"
      ],
      "metadata": {
        "id": "5HGFyp3MTCmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_supervised_trainer.plot_accs()"
      ],
      "metadata": {
        "id": "dtXBij-Um0rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question:\n",
        "\n",
        "- How does the performance of the ResNet trained from scratch compare to the SimCLR-pretrained model?\n",
        "- Why do you think the contrastive learning model outperforms the ResNet trained from scratch?\n",
        "- What does this suggest about the advantages of self-supervised learning when labeled data is scarce?\n"
      ],
      "metadata": {
        "id": "ylBojjn-QvSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plotting TSNE features\n",
        "simclr_feats = []\n",
        "random_feats = []\n",
        "all_labels = []\n",
        "simclr_supervised_trainer.model.eval()\n",
        "simclr_supervised_trainer.model.cuda()\n",
        "\n",
        "supervised_trainer.model.eval()\n",
        "supervised_trainer.model.cuda()\n",
        "for i, (batch_imgs, batch_labels) in tqdm(enumerate(test_data_loader)):\n",
        "    if i == 60:\n",
        "        break\n",
        "    batch_imgs = batch_imgs.to(device)\n",
        "\n",
        "    simclr_feat, out = simclr_supervised_trainer.model(batch_imgs.to('cuda'))\n",
        "    random_feat, out = supervised_trainer.model(batch_imgs.to('cuda'))\n",
        "\n",
        "    simclr_feats.extend(simclr_feat.detach().cpu().numpy())\n",
        "    random_feats.extend(random_feat.detach().cpu().numpy())\n",
        "    all_labels.extend(batch_labels.numpy())"
      ],
      "metadata": {
        "id": "FPRHAi5ZTDRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE Plot for Visualizing Feature Representations\n",
        "\n",
        "This code generates **t-SNE plots** to visualize and compare feature representations of the **test set** learned through two different training approaches: **SimCLR (self-supervised)** and **Supervised training**. t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique that projects high-dimensional features into a 2D space for visualization. Here's what the code does:\n",
        "\n",
        "1. **Feature Reduction**:\n",
        "   - It uses t-SNE to reduce the dimensionality of test set features from both SimCLR and Supervised models into two components, making them suitable for visualization.\n",
        "\n",
        "2. **Visualization**:\n",
        "   - Two side-by-side scatter plots are created:\n",
        "     - The **left plot** shows the t-SNE visualization of test set features learned by the SimCLR model.\n",
        "     - The **right plot** shows the t-SNE visualization of test set features learned by the Supervised model.\n",
        "   - Each point represents a test image in the feature space, and points are color-coded based on their ground-truth labels.\n",
        "\n",
        "3. **Comparison**:\n",
        "   - By comparing the two plots, we can analyze:\n",
        "     - How well each method clusters similar test samples together.\n",
        "     - Whether features learned through SimCLR or Supervised training form distinct and meaningful groupings in the test set feature space.\n",
        "\n",
        "### Importance of t-SNE in This Context:\n",
        "- **Understanding Feature Quality**: It provides an intuitive way to evaluate how well the models separate test data points of different classes.\n",
        "- **Cluster Analysis**: Distinct and compact clusters in the test set indicate high-quality and generalizable feature representations.\n",
        "- **Comparison Tool**: It highlights differences between SimCLR and Supervised learning approaches in terms of their learned test set feature space.\n",
        "\n",
        "### Questions to Consider:\n",
        "1. **How do the clusters in the t-SNE plots compare?**\n",
        "   - Are the clusters tighter and more distinct in one model's test set feature space than the other?\n",
        "\n",
        "2. **What does this tell you about the quality of features learned by SimCLR compared to Supervised learning on the test set?**\n",
        "\n",
        "3. **Do you observe any overlap or poorly separated clusters in either plot? What might this indicate about the limitations of the corresponding model?**\n",
        "\n",
        "4. **Based on the visualization of the test set features, which approach do you think generalizes better to unseen data, and why?**\n",
        "\n",
        "Reflect on these questions as you analyze the t-SNE visualizations to better understand the differences between self-supervised and supervised learning on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### Tasks to Complete:\n",
        "\n",
        "You need to complete the code in following plot function.\n",
        "\n",
        "1. **Initialize t-SNE**:\n",
        "   - Create two instances of the `TSNE` class from `sklearn.manifold` for the two feature sets (`features1` and `features2`).\n",
        "   - Set `n_components=2` to reduce the dimensionality to 2D.\n",
        "   - Use the provided `random_state` argument to ensure reproducibility.\n",
        "\n",
        "2. **Reduce Features**:\n",
        "   - Use the `fit_transform` method of the t-SNE objects to transform the high-dimensional feature sets into 2D representations.\n",
        "   - Store the transformed features in `reduced_features1` and `reduced_features2`.\n",
        "\n",
        "#### Hints:\n",
        "- Use the provided arguments `perplexity` and `random_state` for consistent and meaningful results.\n",
        "- Check the t-SNE documentation for details.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "s0Q36DzASxNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_dual_tsne(features1, features2, labels1, labels2, labels_name1=\"SimCLR\", labels_name2=\"Supervised\", perplexity=30, random_state=42):\n",
        "    \"\"\"\n",
        "    Generate and plot t-SNE visualizations for two feature sets.\n",
        "\n",
        "    Args:\n",
        "        features1 (np.ndarray): First feature set.\n",
        "        features2 (np.ndarray): Second feature set.\n",
        "        labels1 (np.ndarray): Labels for the first feature set.\n",
        "        labels2 (np.ndarray): Labels for the second feature set.\n",
        "        labels_name1 (str): Label for the first feature set in the plot.\n",
        "        labels_name2 (str): Label for the second feature set in the plot.\n",
        "        perplexity (float): Perplexity for t-SNE.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    \"\"\"\n",
        "    #### START ######\n",
        "    ### WRITE CODE HERE ###\n",
        "    reduced_features1 = None\n",
        "    reduced_features2 = None\n",
        "\n",
        "    #### END #####\n",
        "\n",
        "    # Plot t-SNE for SimCLR Features\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter1 = plt.scatter(\n",
        "        reduced_features1[:, 0], reduced_features1[:, 1], c=labels1, cmap='tab10', s=20, alpha=0.7\n",
        "    )\n",
        "    plt.colorbar(scatter1, label='Labels')\n",
        "    plt.title(f't-SNE Visualization of {labels_name1} Features')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "\n",
        "    # Plot t-SNE for Supervised Features\n",
        "    plt.subplot(1, 2, 2)\n",
        "    scatter2 = plt.scatter(\n",
        "        reduced_features2[:, 0], reduced_features2[:, 1], c=labels2, cmap='tab10', s=20, alpha=0.7\n",
        "    )\n",
        "    plt.colorbar(scatter2, label='Labels')\n",
        "    plt.title(f't-SNE Visualization of {labels_name2} Features')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Convert features and labels to numpy arrays\n",
        "simclr_feats_np = np.array(simclr_feats)\n",
        "random_feats_np = np.array(random_feats)\n",
        "all_labels_np = np.array(all_labels)\n",
        "\n",
        "# Call the function to plot the t-SNE plots\n",
        "plot_dual_tsne(simclr_feats_np, random_feats_np, all_labels_np, all_labels_np, labels_name1=\"SimCLR\", labels_name2=\"Supervised\")"
      ],
      "metadata": {
        "id": "hkYsuo3bXXYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKtUAGLPXejs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}